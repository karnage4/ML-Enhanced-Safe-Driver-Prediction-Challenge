{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import time\n",
    "from matplotlib import pyplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# reproducible seed (your ERP)\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (296209, 67)\n",
      "Test shape:  (126948, 66)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train1.csv')\n",
    "test  = pd.read_csv('test.csv')   # used to generate final submission\n",
    "\n",
    "# Identify target and id (adjust names if different)\n",
    "TARGET = 'target'   # change if different\n",
    "ID_COL = 'id'       # change if different\n",
    "\n",
    "# quick shape\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape: \", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical (explicit): ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat']\n",
      "Binary cols: ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin']\n",
      "Numeric cols: ['ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin', 'feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8']\n"
     ]
    }
   ],
   "source": [
    "# heuristics: columns ending with \"_cat\" are categorical (as your assignment says)\n",
    "cat_cols = [c for c in train.columns if c.endswith('_cat')]\n",
    "# But also some integer columns might be binary categorical (0/1)\n",
    "binary_cols = [c for c in train.columns if train[c].dropna().nunique() == 2 and c != TARGET and c != ID_COL and c not in cat_cols]\n",
    "\n",
    "# Continuous numeric columns = numeric columns excluding ID and target and categorical/binary detected above\n",
    "num_cols = [c for c in train.select_dtypes(include=['int64','float64']).columns\n",
    "            if c not in cat_cols + [TARGET, ID_COL]]\n",
    "\n",
    "# If some cat_cols are numeric type, keep them in cat_cols\n",
    "print(\"Categorical (explicit):\", cat_cols)\n",
    "print(\"Binary cols:\", binary_cols)\n",
    "print(\"Numeric cols:\", num_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (222156, 22) X_val (74053, 22)\n"
     ]
    }
   ],
   "source": [
    "#X = train.drop([TARGET, ID_COL], axis=1)\n",
    "selected_features = ['ps_ind_05_cat', 'ps_car_04_cat', 'ps_ind_15', 'ps_reg_01', 'ps_reg_02',\n",
    "                     'ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_15', 'feature2', 'feature4', 'ps_ind_04_cat',\n",
    "                     'ps_car_02_cat', 'ps_car_05_cat', 'ps_car_08_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
    "                     'ps_ind_09_bin', 'ps_ind_12_bin', 'ps_ind_16_bin', 'ps_ind_17_bin']\n",
    "\n",
    "X = train[selected_features]\n",
    "y = train[TARGET]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "print(\"X_train\", X_train.shape, \"X_val\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['ps_ind_05_cat', 'ps_car_04_cat', 'ps_ind_15', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_15', 'feature2', 'feature4']\n",
      "Binary columns: ['ps_ind_04_cat', 'ps_car_02_cat', 'ps_car_05_cat', 'ps_car_08_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_12_bin', 'ps_ind_16_bin', 'ps_ind_17_bin']\n",
      "Categorical columns: []\n"
     ]
    }
   ],
   "source": [
    "# --- Re-identify columns for the reduced dataset ---\n",
    "# Make sure this runs AFTER you select your 22 feature columns\n",
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# If you have binary columns (0/1) that you don't want scaled, you can detect them automatically:\n",
    "binary_cols = [col for col in num_cols if X_train[col].nunique() == 2]\n",
    "\n",
    "# Remove binary cols from numeric list to avoid duplication\n",
    "num_cols = [col for col in num_cols if col not in binary_cols]\n",
    "\n",
    "print(f\"Numeric columns: {num_cols}\")\n",
    "print(f\"Binary columns: {binary_cols}\")\n",
    "print(f\"Categorical columns: {cat_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define imputers ---\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "bin_imputer = SimpleImputer(strategy='most_frequent')  # new addition\n",
    "\n",
    "# --- Combined preprocessing transformers ---\n",
    "\n",
    "# For KNN and Naive Bayes (scaling required)\n",
    "preprocessor_scaled = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', num_imputer),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ]), num_cols),\n",
    "    ('bin', Pipeline([\n",
    "        ('imputer', bin_imputer)  # handle NaNs in binary features\n",
    "    ]), binary_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', cat_imputer),\n",
    "        ('encoder', OrdinalEncoder())\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# For tree-based models (no scaling needed)\n",
    "preprocessor_trees = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', num_imputer)\n",
    "    ]), num_cols),\n",
    "    ('bin', Pipeline([\n",
    "        ('imputer', bin_imputer)\n",
    "    ]), binary_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', cat_imputer),\n",
    "        ('encoder', OrdinalEncoder())\n",
    "    ]), cat_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models defined successfully with preprocessing pipelines.\n"
     ]
    }
   ],
   "source": [
    "# --- Define models with appropriate preprocessing ---\n",
    "models = {\n",
    "    # Distance/probability-based models (require scaling)\n",
    "    'KNN': Pipeline([\n",
    "        ('pre', preprocessor_scaled),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "    ]),\n",
    "    'Naive Bayes': Pipeline([\n",
    "        ('pre', preprocessor_scaled),\n",
    "        ('nb', GaussianNB())\n",
    "    ]),\n",
    "\n",
    "    # Tree-based models (no scaling required)\n",
    "    'Decision Tree': Pipeline([\n",
    "        ('pre', preprocessor_trees),\n",
    "        ('dt', DecisionTreeClassifier(max_depth=7, random_state=42))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('pre', preprocessor_trees),\n",
    "        ('rf', RandomForestClassifier(random_state=42))\n",
    "    ]),\n",
    "    'Extra Trees': Pipeline([\n",
    "        ('pre', preprocessor_trees),\n",
    "        ('et', ExtraTreesClassifier(random_state=42))\n",
    "    ]),\n",
    "    'AdaBoost': Pipeline([\n",
    "        ('pre', preprocessor_trees),\n",
    "        ('ada', AdaBoostClassifier(random_state=42))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('pre', preprocessor_trees),\n",
    "        ('xgb', xgb.XGBClassifier(random_state=42, eval_metric='logloss'))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline([\n",
    "        ('pre', preprocessor_trees),\n",
    "        ('lgb', lgb.LGBMClassifier(random_state=42, verbose=-1))\n",
    "    ]),\n",
    "    'CatBoost': Pipeline([\n",
    "        ('pre', preprocessor_trees),\n",
    "        ('cat', CatBoostClassifier(random_state=42, verbose=0))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Models defined successfully with preprocessing pipelines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight hyperparameter grids aligned with pipeline step names\n",
    "param_distributions = {\n",
    "    'KNN': {\n",
    "        'knn__n_neighbors': [7, 9],\n",
    "        'knn__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        # GaussianNB uses var_smoothing (small positive float)\n",
    "        'nb__var_smoothing': [1e-9, 1e-7]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'dt__max_depth': [5],\n",
    "        'dt__min_samples_leaf': [1],\n",
    "        'dt__min_samples_split': [2],\n",
    "        'dt__criterion': ['entropy']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'rf__n_estimators': [100],\n",
    "        'rf__max_depth': [10, None],\n",
    "        'rf__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'et__n_estimators': [100],\n",
    "        'et__max_depth': [10, None],\n",
    "        'et__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'ada__n_estimators': [50, 100],\n",
    "        'ada__learning_rate': [0.05, 0.1, 0.5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'xgb__n_estimators': [100],\n",
    "        'xgb__max_depth': [3, 6],\n",
    "        'xgb__learning_rate': [0.05, 0.1]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'lgb__n_estimators': [100],\n",
    "        'lgb__num_leaves': [31, 63],\n",
    "        'lgb__learning_rate': [0.05, 0.1]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'cat__iterations': [200],\n",
    "        'cat__depth': [4, 6],\n",
    "        'cat__learning_rate': [0.05, 0.1]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting RandomizedSearchCV for all models...\n",
      "\n",
      "[20:56:23] üîπ Tuning KNN...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "‚úÖ KNN done\n",
      "   Best Params: {'knn__weights': 'distance', 'knn__n_neighbors': 9}\n",
      "   Validation AUROC: 0.5339\n",
      "   Training Time: 585.43s | Prediction Time: 68.40s\n",
      "\n",
      "[21:07:17] üîπ Tuning Naive Bayes...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "‚úÖ Naive Bayes done\n",
      "   Best Params: {'nb__var_smoothing': 1e-07}\n",
      "   Validation AUROC: 0.6075\n",
      "   Training Time: 6.98s | Prediction Time: 0.23s\n",
      "\n",
      "[21:07:24] üîπ Tuning Decision Tree...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "‚úÖ Decision Tree done\n",
      "   Best Params: {'dt__min_samples_split': 2, 'dt__min_samples_leaf': 1, 'dt__max_depth': 5, 'dt__criterion': 'entropy'}\n",
      "   Validation AUROC: 0.6029\n",
      "   Training Time: 8.63s | Prediction Time: 0.11s\n",
      "\n",
      "[21:07:33] üîπ Tuning Random Forest...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "‚úÖ Random Forest done\n",
      "   Best Params: {'rf__n_estimators': 100, 'rf__min_samples_leaf': 1, 'rf__max_depth': 10}\n",
      "   Validation AUROC: 0.6189\n",
      "   Training Time: 559.00s | Prediction Time: 1.28s\n",
      "\n",
      "[21:16:54] üîπ Tuning Extra Trees...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "‚úÖ Extra Trees done\n",
      "   Best Params: {'et__n_estimators': 100, 'et__min_samples_leaf': 1, 'et__max_depth': 10}\n",
      "   Validation AUROC: 0.6164\n",
      "   Training Time: 383.18s | Prediction Time: 1.10s\n",
      "\n",
      "[21:23:18] üîπ Tuning AdaBoost...\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "‚úÖ AdaBoost done\n",
      "   Best Params: {'ada__n_estimators': 100, 'ada__learning_rate': 0.5}\n",
      "   Validation AUROC: 0.6210\n",
      "   Training Time: 351.06s | Prediction Time: 1.54s\n",
      "\n",
      "[21:29:11] üîπ Tuning XGBoost...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "‚úÖ XGBoost done\n",
      "   Best Params: {'xgb__n_estimators': 100, 'xgb__max_depth': 3, 'xgb__learning_rate': 0.1}\n",
      "   Validation AUROC: 0.6233\n",
      "   Training Time: 34.39s | Prediction Time: 0.17s\n",
      "\n",
      "[21:29:45] üîπ Tuning LightGBM...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "‚úÖ LightGBM done\n",
      "   Best Params: {'lgb__num_leaves': 31, 'lgb__n_estimators': 100, 'lgb__learning_rate': 0.05}\n",
      "   Validation AUROC: 0.6242\n",
      "   Training Time: 37.36s | Prediction Time: 0.35s\n",
      "\n",
      "[21:30:23] üîπ Tuning CatBoost...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "‚úÖ CatBoost done\n",
      "   Best Params: {'cat__learning_rate': 0.1, 'cat__iterations': 100, 'cat__depth': 6}\n",
      "   Validation AUROC: 0.6246\n",
      "   Training Time: 110.75s | Prediction Time: 0.29s\n",
      "\n",
      "\n",
      "üìä Model Comparison After Hyperparameter Tuning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>Train Time (s)</th>\n",
       "      <th>Predict Time (s)</th>\n",
       "      <th>Model File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'cat__learning_rate': 0.1, 'cat__iterations':...</td>\n",
       "      <td>0.6246</td>\n",
       "      <td>110.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>best_catboost.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'lgb__num_leaves': 31, 'lgb__n_estimators': 1...</td>\n",
       "      <td>0.6242</td>\n",
       "      <td>37.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>best_lightgbm.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'xgb__n_estimators': 100, 'xgb__max_depth': 3...</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>34.39</td>\n",
       "      <td>0.17</td>\n",
       "      <td>best_xgboost.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>{'ada__n_estimators': 100, 'ada__learning_rate...</td>\n",
       "      <td>0.6210</td>\n",
       "      <td>351.06</td>\n",
       "      <td>1.54</td>\n",
       "      <td>best_adaboost.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'rf__n_estimators': 100, 'rf__min_samples_lea...</td>\n",
       "      <td>0.6189</td>\n",
       "      <td>559.00</td>\n",
       "      <td>1.28</td>\n",
       "      <td>best_random_forest.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>{'et__n_estimators': 100, 'et__min_samples_lea...</td>\n",
       "      <td>0.6164</td>\n",
       "      <td>383.18</td>\n",
       "      <td>1.10</td>\n",
       "      <td>best_extra_trees.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>{'nb__var_smoothing': 1e-07}</td>\n",
       "      <td>0.6075</td>\n",
       "      <td>6.98</td>\n",
       "      <td>0.23</td>\n",
       "      <td>best_naive_bayes.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>{'dt__min_samples_split': 2, 'dt__min_samples_...</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>8.63</td>\n",
       "      <td>0.11</td>\n",
       "      <td>best_decision_tree.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'knn__weights': 'distance', 'knn__n_neighbors...</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>585.43</td>\n",
       "      <td>68.40</td>\n",
       "      <td>best_knn.joblib</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model                                        Best Params   AUROC  \\\n",
       "0       CatBoost  {'cat__learning_rate': 0.1, 'cat__iterations':...  0.6246   \n",
       "1       LightGBM  {'lgb__num_leaves': 31, 'lgb__n_estimators': 1...  0.6242   \n",
       "2        XGBoost  {'xgb__n_estimators': 100, 'xgb__max_depth': 3...  0.6233   \n",
       "3       AdaBoost  {'ada__n_estimators': 100, 'ada__learning_rate...  0.6210   \n",
       "4  Random Forest  {'rf__n_estimators': 100, 'rf__min_samples_lea...  0.6189   \n",
       "5    Extra Trees  {'et__n_estimators': 100, 'et__min_samples_lea...  0.6164   \n",
       "6    Naive Bayes                       {'nb__var_smoothing': 1e-07}  0.6075   \n",
       "7  Decision Tree  {'dt__min_samples_split': 2, 'dt__min_samples_...  0.6029   \n",
       "8            KNN  {'knn__weights': 'distance', 'knn__n_neighbors...  0.5339   \n",
       "\n",
       "   Train Time (s)  Predict Time (s)                 Model File  \n",
       "0          110.75              0.29       best_catboost.joblib  \n",
       "1           37.36              0.35       best_lightgbm.joblib  \n",
       "2           34.39              0.17        best_xgboost.joblib  \n",
       "3          351.06              1.54       best_adaboost.joblib  \n",
       "4          559.00              1.28  best_random_forest.joblib  \n",
       "5          383.18              1.10    best_extra_trees.joblib  \n",
       "6            6.98              0.23    best_naive_bayes.joblib  \n",
       "7            8.63              0.11  best_decision_tree.joblib  \n",
       "8          585.43             68.40            best_knn.joblib  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best tuned model: CatBoost (AUROC = 0.6246)\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import joblib  # for saving trained models\n",
    "joblib.parallel_backend('threading', n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "# --- Model tuning setup ---\n",
    "tuning_results = []\n",
    "\n",
    "print(\"üöÄ Starting RandomizedSearchCV for all models...\\n\")\n",
    "\n",
    "# Loop through models & their param grids\n",
    "for name, model in models.items():\n",
    "    if name not in param_distributions:\n",
    "        print(f\"‚ö†Ô∏è Skipping {name} ‚Äî no param grid defined.\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] üîπ Tuning {name}...\")\n",
    "    start_train = time.time()\n",
    "\n",
    "    # Randomized search setup\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distributions[name],\n",
    "        n_iter=5,                     # fewer iterations for speed\n",
    "        scoring='roc_auc',\n",
    "        cv=3,                         # 3-fold CV (good balance)\n",
    "        random_state=42,\n",
    "        n_jobs=1,                    # parallel processing\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # --- Fit model ---\n",
    "    search.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    # --- Best estimator and params ---\n",
    "    best_model = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    # --- Evaluate on validation set ---\n",
    "    start_pred = time.time()\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_pred = best_model.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_pred = best_model.decision_function(X_val)\n",
    "    pred_time = time.time() - start_pred\n",
    "\n",
    "    auroc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    # --- Print summary ---\n",
    "    print(f\"‚úÖ {name} done\")\n",
    "    print(f\"   Best Params: {best_params}\")\n",
    "    print(f\"   Validation AUROC: {auroc:.4f}\")\n",
    "    print(f\"   Training Time: {train_time:.2f}s | Prediction Time: {pred_time:.2f}s\\n\")\n",
    "\n",
    "    # --- Save best model to file (optional) ---\n",
    "    filename = f\"best_{name.replace(' ', '_').lower()}.joblib\"\n",
    "    joblib.dump(best_model, filename)\n",
    "\n",
    "    # --- Store results ---\n",
    "    tuning_results.append({\n",
    "        'Model': name,\n",
    "        'Best Params': best_params,\n",
    "        'AUROC': round(auroc, 4),\n",
    "        'Train Time (s)': round(train_time, 2),\n",
    "        'Predict Time (s)': round(pred_time, 2),\n",
    "        'Model File': filename\n",
    "    })\n",
    "\n",
    "# --- Results summary ---\n",
    "results_df = pd.DataFrame(tuning_results).sort_values(by='AUROC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüìä Model Comparison After Hyperparameter Tuning:\")\n",
    "display(results_df)\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best tuned model: {best_model_name} (AUROC = {results_df.iloc[0]['AUROC']})\")\n",
    "\n",
    "# Optional: load best model later if needed\n",
    "best_model = joblib.load(results_df.iloc[0]['Model File'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Starting fine-tuned GridSearchCV on top models...\n",
      "\n",
      "üîπ Grid searching CatBoost...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "\n",
      "üìä AUROC for each parameter combination:\n",
      "                                               params  mean_test_score  \\\n",
      "7   {'cat__depth': 6, 'cat__iterations': 200, 'cat...         0.625974   \n",
      "2   {'cat__depth': 6, 'cat__iterations': 100, 'cat...         0.625777   \n",
      "4   {'cat__depth': 6, 'cat__iterations': 150, 'cat...         0.625216   \n",
      "5   {'cat__depth': 6, 'cat__iterations': 150, 'cat...         0.625033   \n",
      "13  {'cat__depth': 7, 'cat__iterations': 150, 'cat...         0.624981   \n",
      "16  {'cat__depth': 7, 'cat__iterations': 200, 'cat...         0.624816   \n",
      "22  {'cat__depth': 8, 'cat__iterations': 150, 'cat...         0.624806   \n",
      "19  {'cat__depth': 8, 'cat__iterations': 100, 'cat...         0.624792   \n",
      "10  {'cat__depth': 7, 'cat__iterations': 100, 'cat...         0.624325   \n",
      "1   {'cat__depth': 6, 'cat__iterations': 100, 'cat...         0.624295   \n",
      "25  {'cat__depth': 8, 'cat__iterations': 200, 'cat...         0.624181   \n",
      "11  {'cat__depth': 7, 'cat__iterations': 100, 'cat...         0.624126   \n",
      "14  {'cat__depth': 7, 'cat__iterations': 150, 'cat...         0.623461   \n",
      "8   {'cat__depth': 6, 'cat__iterations': 200, 'cat...         0.623396   \n",
      "24  {'cat__depth': 8, 'cat__iterations': 200, 'cat...         0.621870   \n",
      "20  {'cat__depth': 8, 'cat__iterations': 100, 'cat...         0.621646   \n",
      "15  {'cat__depth': 7, 'cat__iterations': 200, 'cat...         0.621307   \n",
      "17  {'cat__depth': 7, 'cat__iterations': 200, 'cat...         0.621218   \n",
      "21  {'cat__depth': 8, 'cat__iterations': 150, 'cat...         0.620661   \n",
      "6   {'cat__depth': 6, 'cat__iterations': 200, 'cat...         0.620183   \n",
      "12  {'cat__depth': 7, 'cat__iterations': 150, 'cat...         0.619703   \n",
      "18  {'cat__depth': 8, 'cat__iterations': 100, 'cat...         0.619480   \n",
      "3   {'cat__depth': 6, 'cat__iterations': 150, 'cat...         0.618360   \n",
      "23  {'cat__depth': 8, 'cat__iterations': 150, 'cat...         0.618341   \n",
      "9   {'cat__depth': 7, 'cat__iterations': 100, 'cat...         0.617985   \n",
      "0   {'cat__depth': 6, 'cat__iterations': 100, 'cat...         0.616218   \n",
      "26  {'cat__depth': 8, 'cat__iterations': 200, 'cat...         0.615528   \n",
      "\n",
      "    std_test_score  \n",
      "7         0.003521  \n",
      "2         0.003532  \n",
      "4         0.003730  \n",
      "5         0.003256  \n",
      "13        0.003035  \n",
      "16        0.003140  \n",
      "22        0.002602  \n",
      "19        0.003111  \n",
      "10        0.003113  \n",
      "1         0.003723  \n",
      "25        0.002316  \n",
      "11        0.003196  \n",
      "14        0.002969  \n",
      "8         0.002794  \n",
      "24        0.003989  \n",
      "20        0.002274  \n",
      "15        0.003970  \n",
      "17        0.002569  \n",
      "21        0.003961  \n",
      "6         0.003898  \n",
      "12        0.003992  \n",
      "18        0.003734  \n",
      "3         0.003876  \n",
      "23        0.002672  \n",
      "9         0.004055  \n",
      "0         0.003822  \n",
      "26        0.001765  \n",
      "üíæ Saved detailed results to: grid_results_CatBoost.csv\n",
      "\n",
      "‚úÖ CatBoost ‚Äî Best Params: {'cat__depth': 6, 'cat__iterations': 200, 'cat__learning_rate': 0.05}\n",
      "   AUROC: 0.6253 | Train Time: 1035.95s | Predict Time: 0.27s\n",
      "\n",
      "üîπ Grid searching LightGBM...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "\n",
      "üìä AUROC for each parameter combination:\n",
      "                                               params  mean_test_score  \\\n",
      "9   {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.623686   \n",
      "12  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.622321   \n",
      "6   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.621967   \n",
      "7   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.621715   \n",
      "8   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.621391   \n",
      "15  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.620974   \n",
      "3   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.620887   \n",
      "4   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.620721   \n",
      "10  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.620631   \n",
      "5   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.620524   \n",
      "11  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.620405   \n",
      "18  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.619263   \n",
      "1   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.618941   \n",
      "13  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.618635   \n",
      "2   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.618488   \n",
      "0   {'lgb__learning_rate': 0.01, 'lgb__n_estimator...         0.618451   \n",
      "14  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.618216   \n",
      "16  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.617218   \n",
      "17  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...         0.616642   \n",
      "21  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.615711   \n",
      "19  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.615513   \n",
      "20  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.613798   \n",
      "24  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.613195   \n",
      "22  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.611808   \n",
      "25  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.609075   \n",
      "23  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.608660   \n",
      "26  {'lgb__learning_rate': 0.1, 'lgb__n_estimators...         0.604248   \n",
      "\n",
      "    std_test_score  \n",
      "9         0.004276  \n",
      "12        0.004270  \n",
      "6         0.004062  \n",
      "7         0.003761  \n",
      "8         0.003154  \n",
      "15        0.003869  \n",
      "3         0.004162  \n",
      "4         0.003987  \n",
      "10        0.004065  \n",
      "5         0.003049  \n",
      "11        0.003270  \n",
      "18        0.003511  \n",
      "1         0.004224  \n",
      "13        0.003881  \n",
      "2         0.002988  \n",
      "0         0.004264  \n",
      "14        0.003199  \n",
      "16        0.003565  \n",
      "17        0.003769  \n",
      "21        0.004275  \n",
      "19        0.003422  \n",
      "20        0.004015  \n",
      "24        0.004638  \n",
      "22        0.002997  \n",
      "25        0.002843  \n",
      "23        0.004287  \n",
      "26        0.004365  \n",
      "üíæ Saved detailed results to: grid_results_LightGBM.csv\n",
      "\n",
      "‚úÖ LightGBM ‚Äî Best Params: {'lgb__learning_rate': 0.05, 'lgb__n_estimators': 100, 'lgb__num_leaves': 31}\n",
      "   AUROC: 0.6242 | Train Time: 343.79s | Predict Time: 0.49s\n",
      "\n",
      "üîπ Grid searching XGBoost...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "\n",
      "üìä AUROC for each parameter combination:\n",
      "                                               params  mean_test_score  \\\n",
      "11  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.625537   \n",
      "18  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.625077   \n",
      "10  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.624809   \n",
      "19  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.624717   \n",
      "13  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.624650   \n",
      "14  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.624641   \n",
      "12  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.624621   \n",
      "15  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.623918   \n",
      "9   {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.623668   \n",
      "20  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.623609   \n",
      "21  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.623389   \n",
      "16  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.623079   \n",
      "24  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.622815   \n",
      "22  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.621921   \n",
      "8   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.621767   \n",
      "17  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...         0.621533   \n",
      "5   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.620796   \n",
      "23  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.620774   \n",
      "7   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.620545   \n",
      "25  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.619690   \n",
      "2   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.618838   \n",
      "4   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.618790   \n",
      "6   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.617877   \n",
      "26  {'xgb__learning_rate': 0.1, 'xgb__max_depth': ...         0.616728   \n",
      "1   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.616494   \n",
      "3   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.615656   \n",
      "0   {'xgb__learning_rate': 0.01, 'xgb__max_depth':...         0.613094   \n",
      "\n",
      "    std_test_score  \n",
      "11        0.002686  \n",
      "18        0.002812  \n",
      "10        0.002829  \n",
      "19        0.002499  \n",
      "13        0.003001  \n",
      "14        0.002897  \n",
      "12        0.003382  \n",
      "15        0.002465  \n",
      "9         0.003254  \n",
      "20        0.002203  \n",
      "21        0.002470  \n",
      "16        0.002046  \n",
      "24        0.003398  \n",
      "22        0.002040  \n",
      "8         0.003051  \n",
      "17        0.002563  \n",
      "5         0.003449  \n",
      "23        0.002054  \n",
      "7         0.003185  \n",
      "25        0.002786  \n",
      "2         0.003433  \n",
      "4         0.003381  \n",
      "6         0.003587  \n",
      "26        0.002478  \n",
      "1         0.003309  \n",
      "3         0.003671  \n",
      "0         0.003231  \n",
      "üíæ Saved detailed results to: grid_results_XGBoost.csv\n",
      "\n",
      "‚úÖ XGBoost ‚Äî Best Params: {'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "   AUROC: 0.6243 | Train Time: 254.93s | Predict Time: 0.25s\n",
      "\n",
      "üîπ Grid searching Random Forest...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "üìä AUROC for each parameter combination:\n",
      "                                               params  mean_test_score  \\\n",
      "1   {'rf__max_depth': 10, 'rf__min_samples_leaf': ...         0.622913   \n",
      "3   {'rf__max_depth': 10, 'rf__min_samples_leaf': ...         0.622285   \n",
      "0   {'rf__max_depth': 10, 'rf__min_samples_leaf': ...         0.622205   \n",
      "2   {'rf__max_depth': 10, 'rf__min_samples_leaf': ...         0.621528   \n",
      "5   {'rf__max_depth': 15, 'rf__min_samples_leaf': ...         0.614375   \n",
      "7   {'rf__max_depth': 15, 'rf__min_samples_leaf': ...         0.614212   \n",
      "6   {'rf__max_depth': 15, 'rf__min_samples_leaf': ...         0.610681   \n",
      "4   {'rf__max_depth': 15, 'rf__min_samples_leaf': ...         0.610453   \n",
      "11  {'rf__max_depth': 20, 'rf__min_samples_leaf': ...         0.603161   \n",
      "10  {'rf__max_depth': 20, 'rf__min_samples_leaf': ...         0.597479   \n",
      "9   {'rf__max_depth': 20, 'rf__min_samples_leaf': ...         0.597066   \n",
      "8   {'rf__max_depth': 20, 'rf__min_samples_leaf': ...         0.592985   \n",
      "\n",
      "    std_test_score  \n",
      "1         0.003346  \n",
      "3         0.003223  \n",
      "0         0.002947  \n",
      "2         0.003453  \n",
      "5         0.002691  \n",
      "7         0.002600  \n",
      "6         0.003062  \n",
      "4         0.003328  \n",
      "11        0.002128  \n",
      "10        0.003169  \n",
      "9         0.005200  \n",
      "8         0.006700  \n",
      "üíæ Saved detailed results to: grid_results_Random_Forest.csv\n",
      "\n",
      "‚úÖ Random Forest ‚Äî Best Params: {'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__n_estimators': 200}\n",
      "   AUROC: 0.6201 | Train Time: 2508.03s | Predict Time: 2.75s\n",
      "\n",
      "üîπ Grid searching AdaBoost...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "üìä AUROC for each parameter combination:\n",
      "                                               params  mean_test_score  \\\n",
      "11  {'ada__learning_rate': 0.5, 'ada__n_estimators...         0.619576   \n",
      "10  {'ada__learning_rate': 0.5, 'ada__n_estimators...         0.619332   \n",
      "9   {'ada__learning_rate': 0.5, 'ada__n_estimators...         0.618759   \n",
      "8   {'ada__learning_rate': 0.1, 'ada__n_estimators...         0.617561   \n",
      "7   {'ada__learning_rate': 0.1, 'ada__n_estimators...         0.616437   \n",
      "6   {'ada__learning_rate': 0.1, 'ada__n_estimators...         0.614332   \n",
      "5   {'ada__learning_rate': 0.05, 'ada__n_estimator...         0.613884   \n",
      "4   {'ada__learning_rate': 0.05, 'ada__n_estimator...         0.611922   \n",
      "3   {'ada__learning_rate': 0.05, 'ada__n_estimator...         0.606524   \n",
      "0   {'ada__learning_rate': 0.01, 'ada__n_estimator...         0.500000   \n",
      "1   {'ada__learning_rate': 0.01, 'ada__n_estimator...         0.500000   \n",
      "2   {'ada__learning_rate': 0.01, 'ada__n_estimator...         0.500000   \n",
      "\n",
      "    std_test_score  \n",
      "11        0.005018  \n",
      "10        0.005060  \n",
      "9         0.004878  \n",
      "8         0.004094  \n",
      "7         0.004045  \n",
      "6         0.003474  \n",
      "5         0.003516  \n",
      "4         0.003193  \n",
      "3         0.003539  \n",
      "0         0.000000  \n",
      "1         0.000000  \n",
      "2         0.000000  \n",
      "üíæ Saved detailed results to: grid_results_AdaBoost.csv\n",
      "\n",
      "‚úÖ AdaBoost ‚Äî Best Params: {'ada__learning_rate': 0.5, 'ada__n_estimators': 200}\n",
      "   AUROC: 0.6210 | Train Time: 1895.33s | Predict Time: 3.03s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>Train Time (s)</th>\n",
       "      <th>Predict Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'cat__depth': 6, 'cat__iterations': 200, 'cat...</td>\n",
       "      <td>0.6253</td>\n",
       "      <td>1035.95</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'xgb__learning_rate': 0.05, 'xgb__max_depth':...</td>\n",
       "      <td>0.6243</td>\n",
       "      <td>254.93</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'lgb__learning_rate': 0.05, 'lgb__n_estimator...</td>\n",
       "      <td>0.6242</td>\n",
       "      <td>343.79</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>{'ada__learning_rate': 0.5, 'ada__n_estimators...</td>\n",
       "      <td>0.6210</td>\n",
       "      <td>1895.33</td>\n",
       "      <td>3.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'rf__max_depth': 10, 'rf__min_samples_leaf': ...</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>2508.03</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model                                        Best Params   AUROC  \\\n",
       "0       CatBoost  {'cat__depth': 6, 'cat__iterations': 200, 'cat...  0.6253   \n",
       "2        XGBoost  {'xgb__learning_rate': 0.05, 'xgb__max_depth':...  0.6243   \n",
       "1       LightGBM  {'lgb__learning_rate': 0.05, 'lgb__n_estimator...  0.6242   \n",
       "4       AdaBoost  {'ada__learning_rate': 0.5, 'ada__n_estimators...  0.6210   \n",
       "3  Random Forest  {'rf__max_depth': 10, 'rf__min_samples_leaf': ...  0.6201   \n",
       "\n",
       "   Train Time (s)  Predict Time (s)  \n",
       "0         1035.95              0.27  \n",
       "2          254.93              0.25  \n",
       "1          343.79              0.49  \n",
       "4         1895.33              3.03  \n",
       "3         2508.03              2.75  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved summary of best results to: grid_results_summary.csv\n",
      "\n",
      "üèÜ Final best model after fine-tuning: CatBoost (AUROC = 0.6253)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Narrow grids around best random search results\n",
    "fine_grids = {\n",
    "    'CatBoost': {\n",
    "        'cat__iterations': [100, 150, 200],\n",
    "        'cat__depth': [6, 7, 8],\n",
    "        'cat__learning_rate': [0.01, 0.05, 0.1]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'lgb__n_estimators': [100, 150, 200],\n",
    "        'lgb__num_leaves': [31, 50, 63],\n",
    "        'lgb__learning_rate': [0.01, 0.05, 0.1]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'xgb__n_estimators': [100, 150, 200],\n",
    "        'xgb__max_depth': [3, 4, 5],\n",
    "        'xgb__learning_rate': [0.01, 0.05, 0.1]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'rf__n_estimators': [100, 200],\n",
    "        'rf__max_depth': [10, 15, 20],\n",
    "        'rf__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'ada__n_estimators': [100, 150, 200],\n",
    "        'ada__learning_rate': [0.01, 0.05, 0.1, 0.5]\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_results = []\n",
    "\n",
    "print(\"üéØ Starting fine-tuned GridSearchCV on top models...\\n\")\n",
    "\n",
    "for name in fine_grids.keys():\n",
    "    print(f\"üîπ Grid searching {name}...\")\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=models[name],\n",
    "        param_grid=fine_grids[name],\n",
    "        scoring='roc_auc',\n",
    "        cv=3,\n",
    "        n_jobs=1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    # --- Save AUROC for each parameter combo ---\n",
    "    results_df = pd.DataFrame(grid.cv_results_)\n",
    "    results_df = results_df[['params', 'mean_test_score', 'std_test_score']].sort_values(by='mean_test_score', ascending=False)\n",
    "    print(\"\\nüìä AUROC for each parameter combination:\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Save to individual CSV for this model\n",
    "    csv_name = f\"grid_results_{name.replace(' ', '_')}.csv\"\n",
    "    results_df.to_csv(csv_name, index=False)\n",
    "    print(f\"üíæ Saved detailed results to: {csv_name}\\n\")\n",
    "\n",
    "    # --- Evaluate best estimator ---\n",
    "    best_est = grid.best_estimator_\n",
    "    best_params = grid.best_params_\n",
    "\n",
    "    start_pred = time.time()\n",
    "    y_pred = best_est.predict_proba(X_val)[:, 1]\n",
    "    pred_time = time.time() - start_pred\n",
    "    auroc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"‚úÖ {name} ‚Äî Best Params: {best_params}\")\n",
    "    print(f\"   AUROC: {auroc:.4f} | Train Time: {train_time:.2f}s | Predict Time: {pred_time:.2f}s\\n\")\n",
    "\n",
    "    grid_results.append({\n",
    "        'Model': name,\n",
    "        'Best Params': best_params,\n",
    "        'AUROC': round(auroc, 4),\n",
    "        'Train Time (s)': round(train_time, 2),\n",
    "        'Predict Time (s)': round(pred_time, 2)\n",
    "    })\n",
    "\n",
    "# --- Combine all best results ---\n",
    "grid_results_df = pd.DataFrame(grid_results).sort_values(by='AUROC', ascending=False)\n",
    "display(grid_results_df)\n",
    "\n",
    "# Save combined summary\n",
    "grid_results_df.to_csv(\"grid_results_summary.csv\", index=False)\n",
    "print(\"üíæ Saved summary of best results to: grid_results_summary.csv\")\n",
    "\n",
    "# Print top model\n",
    "best_model_name = grid_results_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Final best model after fine-tuning: {best_model_name} (AUROC = {grid_results_df.iloc[0]['AUROC']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÅ Retraining CatBoost on 100% training data with tuned parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CatBoost training complete in 20.53s\n",
      "‚úÖ Predictions complete in 0.21s\n",
      "üìÅ submission_catboost.csv created successfully!\n",
      "\n",
      "üèÅ Retraining LightGBM on 100% training data with tuned parameters...\n",
      "‚úÖ LightGBM training complete in 7.61s\n",
      "‚úÖ Predictions complete in 0.63s\n",
      "üìÅ submission_lightgbm.csv created successfully!\n",
      "\n",
      "üèÅ Retraining XGBoost on 100% training data with tuned parameters...\n",
      "‚úÖ XGBoost training complete in 6.31s\n",
      "‚úÖ Predictions complete in 0.41s\n",
      "üìÅ submission_xgboost.csv created successfully!\n"
     ]
    }
   ],
   "source": [
    "# === Step 2.7: Final Training and Kaggle Submissions for Top 3 Models ===\n",
    "\n",
    "# Load full train/test datasets\n",
    "train_full = pd.read_csv(\"train1.csv\")\n",
    "test_full = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Separate target\n",
    "y_full = train_full['target']\n",
    "X_full = train_full.drop('target', axis=1)\n",
    "\n",
    "# Use same feature set as in training\n",
    "X_full = X_full[selected_features]\n",
    "test_full = test_full[selected_features]\n",
    "\n",
    "# --- Define the top 3 best parameter sets from your GridSearch results ---\n",
    "best_params_dict = {\n",
    "    \"CatBoost\": {'cat__iterations': 200, 'cat__depth': 6, 'cat__learning_rate': 0.05},\n",
    "    \"LightGBM\": {'lgb__n_estimators': 100, 'lgb__num_leaves': 31, 'lgb__learning_rate': 0.05},\n",
    "    \"XGBoost\": {'xgb__n_estimators': 200, 'xgb__max_depth': 3, 'xgb__learning_rate': 0.05}\n",
    "}\n",
    "\n",
    "# --- Loop over each top model for final training ---\n",
    "for model_name, params in best_params_dict.items():\n",
    "    print(f\"\\nüèÅ Retraining {model_name} on 100% training data with tuned parameters...\")\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.set_params(**params)\n",
    "    \n",
    "    start_train = time.time()\n",
    "    model.fit(X_full, y_full)\n",
    "    train_time = time.time() - start_train\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} training complete in {train_time:.2f}s\")\n",
    "\n",
    "    # Predict on Kaggle test data\n",
    "    start_pred = time.time()\n",
    "    test_preds = model.predict_proba(test_full)[:, 1]\n",
    "    pred_time = time.time() - start_pred\n",
    "\n",
    "    print(f\"‚úÖ Predictions complete in {pred_time:.2f}s\")\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_full.index,   # replace with test_full['id'] if available\n",
    "        'target': test_preds\n",
    "    })\n",
    "    \n",
    "    # Save submission file\n",
    "    filename = f\"submission_{model_name.lower()}.csv\"\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"üìÅ {filename} created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

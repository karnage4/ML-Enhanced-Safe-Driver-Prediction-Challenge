{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Reproducible seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: LOADING DATA\n",
      "======================================================================\n",
      "Train shape: (296209, 67)\n",
      "Test shape:  (126948, 66)\n",
      "Target distribution:\n",
      "target\n",
      "0    0.948732\n",
      "1    0.051268\n",
      "Name: proportion, dtype: float64\n",
      "Class imbalance ratio: 18.51:1\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: LOADING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train = pd.read_csv('train1.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "TARGET = 'target'\n",
    "ID_COL = 'id'\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape:  {test.shape}\")\n",
    "print(f\"Target distribution:\\n{train[TARGET].value_counts(normalize=True)}\")\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "class_ratio = (train[TARGET] == 0).sum() / (train[TARGET] == 1).sum()\n",
    "print(f\"Class imbalance ratio: {class_ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "Applying to train set...\n",
      "Creating missing value indicators...\n",
      "Creating interaction features...\n",
      "Creating polynomial features...\n",
      "Creating aggregation features...\n",
      "Creating binned features...\n",
      "Creating combination features...\n",
      "Feature engineering complete. New shape: (296209, 107)\n",
      "Applying to test set...\n",
      "Creating missing value indicators...\n",
      "Creating interaction features...\n",
      "Creating polynomial features...\n",
      "Creating aggregation features...\n",
      "Creating binned features...\n",
      "Creating combination features...\n",
      "Feature engineering complete. New shape: (126948, 104)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def create_features(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering pipeline\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ===== 2.1: Missing Value Indicators =====\n",
    "    print(\"Creating missing value indicators...\")\n",
    "    missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "    if ID_COL in missing_cols:\n",
    "        missing_cols.remove(ID_COL)\n",
    "    if TARGET in missing_cols and is_train:\n",
    "        missing_cols.remove(TARGET)\n",
    "    \n",
    "    for col in missing_cols:\n",
    "        df[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "    \n",
    "    # ===== 2.2: Interaction Features =====\n",
    "    print(\"Creating interaction features...\")\n",
    "    \n",
    "    # High-value interactions based on domain knowledge\n",
    "    if 'ps_car_13' in df.columns and 'ps_reg_03' in df.columns:\n",
    "        df['car13_reg03_interaction'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    \n",
    "    if 'ps_ind_15' in df.columns and 'ps_reg_01' in df.columns:\n",
    "        df['ind15_reg01_interaction'] = df['ps_ind_15'] * df['ps_reg_01']\n",
    "    \n",
    "    if 'ps_car_13' in df.columns and 'ps_car_15' in df.columns:\n",
    "        df['car13_car15_ratio'] = df['ps_car_13'] / (df['ps_car_15'] + 1e-5)\n",
    "    \n",
    "    if 'ps_reg_02' in df.columns and 'ps_reg_03' in df.columns:\n",
    "        df['reg02_reg03_product'] = df['ps_reg_02'] * df['ps_reg_03']\n",
    "    \n",
    "    # ===== 2.3: Polynomial Features (key variables) =====\n",
    "    print(\"Creating polynomial features...\")\n",
    "    poly_cols = ['ps_car_13', 'ps_reg_03', 'ps_car_15', 'ps_ind_15']\n",
    "    \n",
    "    for col in poly_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_squared'] = df[col] ** 2\n",
    "            df[f'{col}_cubed'] = df[col] ** 3\n",
    "            df[f'{col}_sqrt'] = np.sqrt(np.abs(df[col]))\n",
    "    \n",
    "    # ===== 2.4: Aggregation Features =====\n",
    "    print(\"Creating aggregation features...\")\n",
    "    \n",
    "    # Sum of all car features\n",
    "    car_cols = [c for c in df.columns if c.startswith('ps_car_') and c.endswith('_cat')]\n",
    "    if car_cols:\n",
    "        df['car_cat_sum'] = df[car_cols].sum(axis=1)\n",
    "        df['car_cat_mean'] = df[car_cols].mean(axis=1)\n",
    "    \n",
    "    # Sum of all ind features\n",
    "    ind_cols = [c for c in df.columns if c.startswith('ps_ind_') and c.endswith('_bin')]\n",
    "    if ind_cols:\n",
    "        df['ind_bin_sum'] = df[ind_cols].sum(axis=1)\n",
    "    \n",
    "    # Sum of all calc features\n",
    "    calc_cols = [c for c in df.columns if c.startswith('ps_calc_')]\n",
    "    if calc_cols:\n",
    "        df['calc_sum'] = df[calc_cols].sum(axis=1)\n",
    "        df['calc_mean'] = df[calc_cols].mean(axis=1)\n",
    "        df['calc_std'] = df[calc_cols].std(axis=1)\n",
    "    \n",
    "    # ===== 2.5: Binning Continuous Variables =====\n",
    "    print(\"Creating binned features...\")\n",
    "    \n",
    "    if 'ps_reg_03' in df.columns:\n",
    "        df['ps_reg_03_binned'] = pd.qcut(df['ps_reg_03'].fillna(-1), q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    if 'ps_car_13' in df.columns:\n",
    "        df['ps_car_13_binned'] = pd.qcut(df['ps_car_13'].fillna(-1), q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    # ===== 2.6: Combination Features =====\n",
    "    print(\"Creating combination features...\")\n",
    "    \n",
    "    if 'ps_ind_06_bin' in df.columns and 'ps_ind_07_bin' in df.columns:\n",
    "        df['ind_06_07_combined'] = df['ps_ind_06_bin'].astype(str) + '_' + df['ps_ind_07_bin'].astype(str)\n",
    "        df['ind_06_07_combined'] = LabelEncoder().fit_transform(df['ind_06_07_combined'])\n",
    "    \n",
    "    if 'ps_car_01_cat' in df.columns and 'ps_car_02_cat' in df.columns:\n",
    "        df['car_01_02_combined'] = df['ps_car_01_cat'].astype(str) + '_' + df['ps_car_02_cat'].astype(str)\n",
    "        df['car_01_02_combined'] = LabelEncoder().fit_transform(df['car_01_02_combined'])\n",
    "    \n",
    "    print(f\"Feature engineering complete. New shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\nApplying to train set...\")\n",
    "train_fe = create_features(train, is_train=True)\n",
    "\n",
    "print(\"Applying to test set...\")\n",
    "test_fe = create_features(test, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: FEATURE SELECTION\n",
      "======================================================================\n",
      "Calculating feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\SHEIKHANI LAPTOP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 247, in _count_physical_cores\n",
      "    cpu_count_physical = _count_physical_cores_win32()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SHEIKHANI LAPTOP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 299, in _count_physical_cores_win32\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 30 most important features:\n",
      "                     feature  importance\n",
      "62                  feature6         144\n",
      "34                 ps_car_13         136\n",
      "15                 ps_ind_03         125\n",
      "63                  feature7         117\n",
      "98                  calc_sum         110\n",
      "35                 ps_car_14         104\n",
      "100                 calc_std          99\n",
      "81         car13_car15_ratio          96\n",
      "58                  feature2          91\n",
      "25                 ps_ind_15          90\n",
      "96              car_cat_mean          83\n",
      "80   ind15_reg01_interaction          83\n",
      "2              ps_ind_05_cat          77\n",
      "60                  feature4          72\n",
      "79   car13_reg03_interaction          69\n",
      "104       car_01_02_combined          66\n",
      "31                 ps_reg_03          66\n",
      "29                 ps_reg_01          65\n",
      "95               car_cat_sum          62\n",
      "82       reg02_reg03_product          59\n",
      "37                ps_calc_01          49\n",
      "13             ps_car_11_cat          48\n",
      "36                 ps_car_15          47\n",
      "11             ps_car_09_cat          45\n",
      "38                ps_calc_02          43\n",
      "33                 ps_car_12          41\n",
      "42                ps_calc_06          38\n",
      "43                ps_calc_07          37\n",
      "39                ps_calc_03          37\n",
      "0              ps_ind_02_cat          37\n",
      "\n",
      "Selected 60 features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: FEATURE SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Separate features and target\n",
    "X_full = train_fe.drop([TARGET, ID_COL], axis=1)\n",
    "y_full = train_fe[TARGET]\n",
    "test_final = test_fe.drop([ID_COL], axis=1, errors='ignore')\n",
    "\n",
    "# Fill missing values temporarily for feature selection\n",
    "X_full_filled = X_full.fillna(-999)\n",
    "test_final_filled = test_final.fillna(-999)\n",
    "\n",
    "# Quick feature importance with LightGBM\n",
    "print(\"Calculating feature importance...\")\n",
    "lgb_selector = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "lgb_selector.fit(X_full_filled, y_full)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_full.columns,\n",
    "    'importance': lgb_selector.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 30 most important features:\")\n",
    "print(feature_importance.head(30))\n",
    "\n",
    "# Select top N features (keeping more features now due to engineering)\n",
    "N_FEATURES = 60  # Increased from 22\n",
    "selected_features = feature_importance.head(N_FEATURES)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} features\")\n",
    "\n",
    "# Apply selection\n",
    "X_full = X_full[selected_features]\n",
    "test_final = test_final[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: HANDLING MISSING VALUES\n",
      "======================================================================\n",
      "Missing values in train: 0\n",
      "Missing values in test: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For tree-based models, -999 is often better than mean/median\n",
    "X_full = X_full.fillna(-999)\n",
    "test_final = test_final.fillna(-999)\n",
    "\n",
    "print(f\"Missing values in train: {X_full.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test: {test_final.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: CREATING TRAIN/VALIDATION SPLIT\n",
      "======================================================================\n",
      "X_train: (222156, 60)\n",
      "X_val: (74053, 60)\n",
      "Target distribution in train: {0: 0.9487342227983939, 1: 0.05126577720160608}\n",
      "Target distribution in val: {0: 0.9487259125221126, 1: 0.05127408747788746}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: CREATING TRAIN/VALIDATION SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, \n",
    "    test_size=0.25, \n",
    "    stratify=y_full, \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"Target distribution in train: {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"Target distribution in val: {y_val.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: DEFINING MODELS WITH CLASS WEIGHTS\n",
      "======================================================================\n",
      "Scale pos weight: 18.51\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: DEFINING MODELS WITH CLASS WEIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost/LightGBM\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "models = {\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=RANDOM_SEED,\n",
    "        eval_metric='auc',\n",
    "        tree_method='hist'\n",
    "    ),\n",
    "    'LightGBM': lgb.LGBMClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED,\n",
    "        verbose=-1,\n",
    "        metric='auc'\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        auto_class_weights='Balanced',\n",
    "        random_state=RANDOM_SEED,\n",
    "        verbose=0,\n",
    "        eval_metric='AUC'\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 7: HYPERPARAMETER TUNING (EXPANDED GRIDS)\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "Tuning XGBoost...\n",
      "==================================================\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "==================================================\n",
      "‚úÖ XGBoost Results:\n",
      "==================================================\n",
      "Best Params: {'subsample': 0.9, 'n_estimators': 700, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "Validation AUROC: 0.6353\n",
      "CV AUROC: 0.6352 (+/- 0.0028)\n",
      "Train Time: 2286.07s | Predict Time: 0.72s\n",
      "\n",
      "==================================================\n",
      "Tuning LightGBM...\n",
      "==================================================\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "==================================================\n",
      "‚úÖ LightGBM Results:\n",
      "==================================================\n",
      "Best Params: {'subsample': 0.9, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 31, 'n_estimators': 500, 'min_child_samples': 50, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "Validation AUROC: 0.6359\n",
      "CV AUROC: 0.6340 (+/- 0.0024)\n",
      "Train Time: 4250.89s | Predict Time: 2.00s\n",
      "\n",
      "==================================================\n",
      "Tuning CatBoost...\n",
      "==================================================\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "==================================================\n",
      "‚úÖ CatBoost Results:\n",
      "==================================================\n",
      "Best Params: {'learning_rate': 0.03, 'l2_leaf_reg': 7, 'iterations': 500, 'depth': 4, 'border_count': 128, 'bagging_temperature': 0}\n",
      "Validation AUROC: 0.6378\n",
      "CV AUROC: 0.6357 (+/- 0.0020)\n",
      "Train Time: 10877.82s | Predict Time: 0.21s\n",
      "\n",
      "======================================================================\n",
      "TUNING RESULTS SUMMARY:\n",
      "======================================================================\n",
      "   Model                                                                                                                                                             Best Params  Val AUROC  CV AUROC Mean  CV AUROC Std  Train Time (s)\n",
      "CatBoost                                                 {'learning_rate': 0.03, 'l2_leaf_reg': 7, 'iterations': 500, 'depth': 4, 'border_count': 128, 'bagging_temperature': 0}     0.6378         0.6357        0.0020        10877.82\n",
      "LightGBM {'subsample': 0.9, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 31, 'n_estimators': 500, 'min_child_samples': 50, 'learning_rate': 0.01, 'colsample_bytree': 0.8}     0.6359         0.6340        0.0024         4250.89\n",
      " XGBoost                            {'subsample': 0.9, 'n_estimators': 700, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.8}     0.6353         0.6352        0.0028         2286.07\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 7: HYPERPARAMETER TUNING (EXPANDED GRIDS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "param_grids = {\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [300, 500, 700],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.07],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [300, 500, 700],\n",
    "        'num_leaves': [31, 50, 70, 90],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.07],\n",
    "        'min_child_samples': [20, 30, 50],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [0, 0.1, 0.5]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [300, 500, 700],\n",
    "        'depth': [4, 6, 8, 10],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.07],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7],\n",
    "        'border_count': [32, 64, 128],\n",
    "        'bagging_temperature': [0, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Randomized search for efficiency\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "tuning_results = []\n",
    "best_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Tuning {name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grids[name],\n",
    "        n_iter=30,  # Test 30 random combinations\n",
    "        scoring='roc_auc',\n",
    "        cv=5,  # 5-fold CV for robustness\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    search.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "    \n",
    "    # Evaluate on validation\n",
    "    start_pred = time.time()\n",
    "    y_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    pred_time = time.time() - start_pred\n",
    "    \n",
    "    val_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Cross-validation score for reliability\n",
    "    cv_scores = cross_val_score(\n",
    "        best_model, X_train, y_train,\n",
    "        cv=5, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"‚úÖ {name} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Best Params: {best_params}\")\n",
    "    print(f\"Validation AUROC: {val_auc:.4f}\")\n",
    "    print(f\"CV AUROC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"Train Time: {train_time:.2f}s | Predict Time: {pred_time:.2f}s\")\n",
    "    \n",
    "    # Save results\n",
    "    tuning_results.append({\n",
    "        'Model': name,\n",
    "        'Best Params': best_params,\n",
    "        'Val AUROC': round(val_auc, 4),\n",
    "        'CV AUROC Mean': round(cv_scores.mean(), 4),\n",
    "        'CV AUROC Std': round(cv_scores.std(), 4),\n",
    "        'Train Time (s)': round(train_time, 2)\n",
    "    })\n",
    "    \n",
    "    # Store best model\n",
    "    best_models[name] = best_model\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(best_model, f'best_{name.lower()}_enhanced.joblib')\n",
    "\n",
    "# Results DataFrame\n",
    "results_df = pd.DataFrame(tuning_results).sort_values('Val AUROC', ascending=False)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TUNING RESULTS SUMMARY:\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 8: CREATING STACKING ENSEMBLE\n",
      "======================================================================\n",
      "Training stacking ensemble...\n",
      "\n",
      "==================================================\n",
      "‚úÖ STACKING ENSEMBLE Results:\n",
      "==================================================\n",
      "Validation AUROC: 0.6374\n",
      "CV AUROC: 0.6363 (+/- 0.0022)\n",
      "Train Time: 504.51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_stacking_ensemble.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 8: CREATING STACKING ENSEMBLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create base estimators from best models\n",
    "estimators = [\n",
    "    ('xgb', best_models['XGBoost']),\n",
    "    ('lgb', best_models['LightGBM']),\n",
    "    ('cat', best_models['CatBoost'])\n",
    "]\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED,\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training stacking ensemble...\")\n",
    "start = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "stack_train_time = time.time() - start\n",
    "\n",
    "# Evaluate stacking\n",
    "y_pred_stack = stacking_model.predict_proba(X_val)[:, 1]\n",
    "stack_auc = roc_auc_score(y_val, y_pred_stack)\n",
    "\n",
    "# Cross-validation\n",
    "stack_cv_scores = cross_val_score(\n",
    "    stacking_model, X_train, y_train,\n",
    "    cv=5, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"‚úÖ STACKING ENSEMBLE Results:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Validation AUROC: {stack_auc:.4f}\")\n",
    "print(f\"CV AUROC: {stack_cv_scores.mean():.4f} (+/- {stack_cv_scores.std():.4f})\")\n",
    "print(f\"Train Time: {stack_train_time:.2f}s\")\n",
    "\n",
    "# Save stacking model\n",
    "joblib.dump(stacking_model, 'best_stacking_ensemble.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 9: CREATING WEIGHTED ENSEMBLE\n",
      "======================================================================\n",
      "XGBoost Val AUC: 0.6353\n",
      "LightGBM Val AUC: 0.6359\n",
      "CatBoost Val AUC: 0.6378\n",
      "\n",
      "Simple Average AUC: 0.6373\n",
      "Weighted Average AUC: 0.6373\n",
      "Weights: XGB=0.333, LGB=0.333, CAT=0.334\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 9: CREATING WEIGHTED ENSEMBLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get predictions from all models\n",
    "pred_xgb = best_models['XGBoost'].predict_proba(X_val)[:, 1]\n",
    "pred_lgb = best_models['LightGBM'].predict_proba(X_val)[:, 1]\n",
    "pred_cat = best_models['CatBoost'].predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate individual AUC scores\n",
    "auc_xgb = roc_auc_score(y_val, pred_xgb)\n",
    "auc_lgb = roc_auc_score(y_val, pred_lgb)\n",
    "auc_cat = roc_auc_score(y_val, pred_cat)\n",
    "\n",
    "print(f\"XGBoost Val AUC: {auc_xgb:.4f}\")\n",
    "print(f\"LightGBM Val AUC: {auc_lgb:.4f}\")\n",
    "print(f\"CatBoost Val AUC: {auc_cat:.4f}\")\n",
    "\n",
    "# Simple average\n",
    "pred_avg = (pred_xgb + pred_lgb + pred_cat) / 3\n",
    "auc_avg = roc_auc_score(y_val, pred_avg)\n",
    "print(f\"\\nSimple Average AUC: {auc_avg:.4f}\")\n",
    "\n",
    "# Weighted by performance\n",
    "total_auc = auc_xgb + auc_lgb + auc_cat\n",
    "w_xgb = auc_xgb / total_auc\n",
    "w_lgb = auc_lgb / total_auc\n",
    "w_cat = auc_cat / total_auc\n",
    "\n",
    "pred_weighted = w_xgb * pred_xgb + w_lgb * pred_lgb + w_cat * pred_cat\n",
    "auc_weighted = roc_auc_score(y_val, pred_weighted)\n",
    "print(f\"Weighted Average AUC: {auc_weighted:.4f}\")\n",
    "print(f\"Weights: XGB={w_xgb:.3f}, LGB={w_lgb:.3f}, CAT={w_cat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 10: FINAL MODEL SELECTION\n",
      "======================================================================\n",
      "\n",
      "Final Validation Scores:\n",
      "  CatBoost            : 0.6378\n",
      "  Stacking            : 0.6374\n",
      "  Weighted Average    : 0.6373\n",
      "  Simple Average      : 0.6373\n",
      "  LightGBM            : 0.6359\n",
      "  XGBoost             : 0.6353\n",
      "\n",
      "üèÜ BEST APPROACH: CatBoost (AUROC = 0.6378)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 10: FINAL MODEL SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compare all approaches\n",
    "final_scores = {\n",
    "    'XGBoost': auc_xgb,\n",
    "    'LightGBM': auc_lgb,\n",
    "    'CatBoost': auc_cat,\n",
    "    'Stacking': stack_auc,\n",
    "    'Simple Average': auc_avg,\n",
    "    'Weighted Average': auc_weighted\n",
    "}\n",
    "\n",
    "best_approach = max(final_scores, key=final_scores.get)\n",
    "best_score = final_scores[best_approach]\n",
    "\n",
    "print(\"\\nFinal Validation Scores:\")\n",
    "for approach, score in sorted(final_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {approach:20s}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST APPROACH: {best_approach} (AUROC = {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 11: RETRAINING ON FULL DATA & CREATING SUBMISSIONS\n",
      "======================================================================\n",
      "\n",
      "Retraining XGBoost on full training data...\n",
      "\n",
      "Retraining LightGBM on full training data...\n",
      "\n",
      "Retraining CatBoost on full training data...\n",
      "\n",
      "Retraining Stacking Ensemble on full training data...\n",
      "\n",
      "Generating predictions on test set...\n",
      "‚úÖ Created: submission_xgboost_enhanced.csv\n",
      "‚úÖ Created: submission_lightgbm_enhanced.csv\n",
      "‚úÖ Created: submission_catboost_enhanced.csv\n",
      "‚úÖ Created: submission_stacking_enhanced.csv\n",
      "‚úÖ Created: submission_simple_average_enhanced.csv\n",
      "‚úÖ Created: submission_weighted_average_enhanced.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 11: RETRAINING ON FULL DATA & CREATING SUBMISSIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Retrain best individual models on full data\n",
    "final_models = {}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nRetraining {name} on full training data...\")\n",
    "    model.fit(X_full, y_full)\n",
    "    final_models[name] = model\n",
    "\n",
    "# Retrain stacking on full data\n",
    "print(\"\\nRetraining Stacking Ensemble on full training data...\")\n",
    "stacking_model.fit(X_full, y_full)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "\n",
    "pred_test_xgb = final_models['XGBoost'].predict_proba(test_final)[:, 1]\n",
    "pred_test_lgb = final_models['LightGBM'].predict_proba(test_final)[:, 1]\n",
    "pred_test_cat = final_models['CatBoost'].predict_proba(test_final)[:, 1]\n",
    "pred_test_stack = stacking_model.predict_proba(test_final)[:, 1]\n",
    "pred_test_avg = (pred_test_xgb + pred_test_lgb + pred_test_cat) / 3\n",
    "pred_test_weighted = w_xgb * pred_test_xgb + w_lgb * pred_test_lgb + w_cat * pred_test_cat\n",
    "\n",
    "# Create submissions\n",
    "submissions = {\n",
    "    'xgboost': pred_test_xgb,\n",
    "    'lightgbm': pred_test_lgb,\n",
    "    'catboost': pred_test_cat,\n",
    "    'stacking': pred_test_stack,\n",
    "    'simple_average': pred_test_avg,\n",
    "    'weighted_average': pred_test_weighted\n",
    "}\n",
    "\n",
    "test_ids = test[ID_COL] if ID_COL in test.columns else range(len(test))\n",
    "\n",
    "for name, predictions in submissions.items():\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'target': predictions\n",
    "    })\n",
    "    filename = f'submission_{name}_enhanced.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Created: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéâ PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìä Final Statistics:\n",
      "  - Features engineered: 60 (from original ~65)\n",
      "  - Best validation AUROC: 0.6378\n",
      "  - Best approach: CatBoost\n",
      "  - Submissions created: 6\n",
      "\n",
      "üí° Recommendations:\n",
      "  1. Submit 'catboost_enhanced.csv' first\n",
      "  2. Try 'weighted_average_enhanced.csv' as alternative\n",
      "  3. Monitor Kaggle leaderboard scores\n",
      "  4. If score < 0.65, consider:\n",
      "     - More feature engineering iterations\n",
      "     - Deeper hyperparameter tuning\n",
      "     - Neural network models\n",
      "     - Advanced ensembling techniques\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Final Statistics:\")\n",
    "print(f\"  - Features engineered: {X_full.shape[1]} (from original ~65)\")\n",
    "print(f\"  - Best validation AUROC: {best_score:.4f}\")\n",
    "print(f\"  - Best approach: {best_approach}\")\n",
    "print(f\"  - Submissions created: {len(submissions)}\")\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "print(f\"  1. Submit '{best_approach.lower().replace(' ', '_')}_enhanced.csv' first\")\n",
    "print(f\"  2. Try 'weighted_average_enhanced.csv' as alternative\")\n",
    "print(f\"  3. Monitor Kaggle leaderboard scores\")\n",
    "print(f\"  4. If score < 0.65, consider:\")\n",
    "print(f\"     - More feature engineering iterations\")\n",
    "print(f\"     - Deeper hyperparameter tuning\")\n",
    "print(f\"     - Neural network models\")\n",
    "print(f\"     - Advanced ensembling techniques\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

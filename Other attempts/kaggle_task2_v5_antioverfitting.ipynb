{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANTI-OVERFITTING PIPELINE\n",
      "Addressing: Val=0.651 vs Kaggle=0.642 gap\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANTI-OVERFITTING PIPELINE\")\n",
    "print(\"Addressing: Val=0.651 vs Kaggle=0.642 gap\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: ADVERSARIAL VALIDATION\n",
      "================================================================================\n",
      "Training adversarial model to detect train/test differences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\SHEIKHANI LAPTOP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 247, in _count_physical_cores\n",
      "    cpu_count_physical = _count_physical_cores_win32()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SHEIKHANI LAPTOP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 299, in _count_physical_cores_win32\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial AUC: 0.4979\n",
      "Interpretation:\n",
      "  ‚úÖ Train and test distributions are very similar\n",
      "\n",
      "Top 10 features causing train/test mismatch:\n",
      "          feature  importance\n",
      "54       feature7         195\n",
      "18      ps_car_13         161\n",
      "64       feature6         150\n",
      "59      ps_reg_03         149\n",
      "40      ps_car_14         148\n",
      "27       feature4         143\n",
      "48       feature2         111\n",
      "53  ps_car_11_cat         107\n",
      "63      ps_ind_15          95\n",
      "24     ps_calc_02          88\n",
      "\n",
      "Problematic features to monitor: 7\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 1: ADVERSARIAL VALIDATION - Detect Distribution Shift\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: ADVERSARIAL VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train = pd.read_csv('train1.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "TARGET = 'target'\n",
    "ID_COL = 'id'\n",
    "\n",
    "# Create adversarial dataset\n",
    "train_adv = train.drop([TARGET, ID_COL], axis=1, errors='ignore').copy()\n",
    "test_adv = test.drop([ID_COL], axis=1, errors='ignore').copy()\n",
    "\n",
    "train_adv['is_test'] = 0\n",
    "test_adv['is_test'] = 1\n",
    "\n",
    "# Align columns\n",
    "common_cols = list(set(train_adv.columns) & set(test_adv.columns))\n",
    "train_adv = train_adv[common_cols]\n",
    "test_adv = test_adv[common_cols]\n",
    "\n",
    "adv_data = pd.concat([train_adv, test_adv], axis=0).reset_index(drop=True)\n",
    "adv_target = adv_data['is_test']\n",
    "adv_features = adv_data.drop('is_test', axis=1).fillna(-999)\n",
    "\n",
    "# Train adversarial model\n",
    "print(\"Training adversarial model to detect train/test differences...\")\n",
    "adv_model = lgb.LGBMClassifier(n_estimators=100, random_state=RANDOM_SEED, verbose=-1)\n",
    "adv_scores = cross_val_score(adv_model, adv_features, adv_target, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Adversarial AUC: {adv_scores.mean():.4f}\")\n",
    "print(\"Interpretation:\")\n",
    "if adv_scores.mean() < 0.55:\n",
    "    print(\"  ‚úÖ Train and test distributions are very similar\")\n",
    "elif adv_scores.mean() < 0.65:\n",
    "    print(\"  ‚ö†Ô∏è  Moderate distribution shift detected\")\n",
    "else:\n",
    "    print(\"  üö® Significant distribution shift - high overfitting risk!\")\n",
    "\n",
    "# Get feature importance from adversarial model\n",
    "adv_model.fit(adv_features, adv_target)\n",
    "adv_importance = pd.DataFrame({\n",
    "    'feature': adv_features.columns,\n",
    "    'importance': adv_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features causing train/test mismatch:\")\n",
    "print(adv_importance.head(10))\n",
    "\n",
    "# Identify problematic features (high adversarial importance)\n",
    "problematic_features = adv_importance[adv_importance['importance'] > adv_importance['importance'].quantile(0.9)]['feature'].tolist()\n",
    "print(f\"\\nProblematic features to monitor: {len(problematic_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: CONSERVATIVE FEATURE ENGINEERING\n",
      "================================================================================\n",
      "Applying to train...\n",
      "  Creating robust features...\n",
      "Applying to test...\n",
      "  Creating robust features...\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 2: CONSERVATIVE FEATURE ENGINEERING\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: CONSERVATIVE FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def create_conservative_features(df, is_train=True):\n",
    "    \"\"\"\n",
    "    More conservative feature engineering - avoid overfitting\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"  Creating robust features...\")\n",
    "    \n",
    "    # Only simple, stable interactions\n",
    "    if 'ps_car_13' in df.columns and 'ps_reg_03' in df.columns:\n",
    "        df['car13_reg03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    \n",
    "    if 'ps_ind_15' in df.columns and 'ps_reg_01' in df.columns:\n",
    "        df['ind15_reg01'] = df['ps_ind_15'] * df['ps_reg_01']\n",
    "    \n",
    "    # Simple aggregations only\n",
    "    car_cols = [c for c in df.columns if c.startswith('ps_car_') and c.endswith('_cat')]\n",
    "    if car_cols:\n",
    "        df['car_cat_sum'] = df[car_cols].sum(axis=1)\n",
    "    \n",
    "    ind_cols = [c for c in df.columns if c.startswith('ps_ind_') and c.endswith('_bin')]\n",
    "    if ind_cols:\n",
    "        df['ind_bin_sum'] = df[ind_cols].sum(axis=1)\n",
    "    \n",
    "    # Missing indicators for key features only\n",
    "    key_missing = ['ps_car_03_cat', 'ps_car_05_cat', 'ps_reg_03', 'ps_car_11', 'ps_car_14']\n",
    "    for col in key_missing:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "    \n",
    "    # Avoid high-order polynomials and complex interactions\n",
    "    # They overfit easily\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Applying to train...\")\n",
    "train_fe = create_conservative_features(train, is_train=True)\n",
    "\n",
    "print(\"Applying to test...\")\n",
    "test_fe = create_conservative_features(test, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: ROBUST CROSS-VALIDATION\n",
      "================================================================================\n",
      "Train features: 74\n",
      "Test features: 74\n",
      "Common features: 74\n",
      "Aligned - Train: 74, Test: 74\n",
      "\n",
      "Dropping 7 problematic features:\n",
      "['feature7', 'ps_car_13', 'feature6', 'ps_reg_03', 'ps_car_14'] ...\n",
      "\n",
      "Final feature count: 67\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 3: ROBUST CROSS-VALIDATION STRATEGY\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: ROBUST CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "X_full = train_fe.drop([TARGET, ID_COL], axis=1)\n",
    "y_full = train_fe[TARGET]\n",
    "test_full = test_fe.drop([ID_COL], axis=1, errors='ignore')\n",
    "\n",
    "# CRITICAL: Align columns between train and test\n",
    "print(f\"Train features: {X_full.shape[1]}\")\n",
    "print(f\"Test features: {test_full.shape[1]}\")\n",
    "\n",
    "# Ensure exact same features\n",
    "common_features = list(set(X_full.columns) & set(test_full.columns))\n",
    "print(f\"Common features: {len(common_features)}\")\n",
    "\n",
    "X_full = X_full[common_features]\n",
    "test_full = test_full[common_features]\n",
    "\n",
    "print(f\"Aligned - Train: {X_full.shape[1]}, Test: {test_full.shape[1]}\")\n",
    "\n",
    "# Fill missing\n",
    "X_full = X_full.fillna(-999)\n",
    "test_full = test_full.fillna(-999)\n",
    "\n",
    "# Remove highly adversarial features\n",
    "features_to_drop = [f for f in problematic_features if f in X_full.columns]\n",
    "if features_to_drop:\n",
    "    print(f\"\\nDropping {len(features_to_drop)} problematic features:\")\n",
    "    print(features_to_drop[:5], \"...\")\n",
    "    X_full = X_full.drop(features_to_drop, axis=1)\n",
    "    test_full = test_full.drop(features_to_drop, axis=1)\n",
    "\n",
    "print(f\"\\nFinal feature count: {X_full.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: CONSERVATIVE TARGET ENCODING\n",
      "================================================================================\n",
      "Applying conservative target encoding to 14 features...\n",
      "  Encoding: ps_car_04_cat\n",
      "  Encoding: ps_car_08_cat\n",
      "  Encoding: ps_car_03_cat\n",
      "  Encoding: ps_ind_05_cat\n",
      "  Encoding: ps_ind_04_cat\n",
      "  Encoding: ps_car_05_cat\n",
      "  Encoding: ps_car_01_cat\n",
      "  Encoding: ps_car_06_cat\n",
      "  Encoding: ps_car_09_cat\n",
      "  Encoding: ps_car_10_cat\n",
      "  Encoding: ps_car_07_cat\n",
      "  Encoding: ps_car_02_cat\n",
      "  Encoding: ps_car_11_cat\n",
      "  Encoding: ps_ind_02_cat\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 4: CONSERVATIVE TARGET ENCODING WITH MORE SMOOTHING\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: CONSERVATIVE TARGET ENCODING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def conservative_target_encode(X_train, y_train, X_test, cat_cols, alpha=20):\n",
    "    \"\"\"\n",
    "    Target encoding with HIGH smoothing to prevent overfitting\n",
    "    alpha=20 (vs 10 before) = more regularization\n",
    "    \"\"\"\n",
    "    X_train_te = X_train.copy()\n",
    "    X_test_te = X_test.copy()\n",
    "    \n",
    "    global_mean = y_train.mean()\n",
    "    \n",
    "    # Use stratified 10-fold (more folds = less overfitting)\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col not in X_train.columns:\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Encoding: {col}\")\n",
    "        \n",
    "        X_train_te[f'{col}_te'] = global_mean  # Initialize with global mean\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train, y_train):\n",
    "            X_tr = X_train.iloc[train_idx]\n",
    "            y_tr = y_train.iloc[train_idx]\n",
    "            X_vl = X_train.iloc[val_idx]\n",
    "            \n",
    "            agg = pd.DataFrame({'col': X_tr[col], 'target': y_tr})\n",
    "            means = agg.groupby('col')['target'].agg(['mean', 'count'])\n",
    "            \n",
    "            # Higher alpha = more smoothing towards global mean\n",
    "            smoothed = (means['count'] * means['mean'] + alpha * global_mean) / (means['count'] + alpha)\n",
    "            \n",
    "            X_train_te.loc[X_train.index[val_idx], f'{col}_te'] = X_vl[col].map(smoothed).fillna(global_mean).values\n",
    "        \n",
    "        # For test, use full training data\n",
    "        agg_full = pd.DataFrame({'col': X_train[col], 'target': y_train})\n",
    "        means_full = agg_full.groupby('col')['target'].agg(['mean', 'count'])\n",
    "        smoothed_full = (means_full['count'] * means_full['mean'] + alpha * global_mean) / (means_full['count'] + alpha)\n",
    "        \n",
    "        X_test_te[f'{col}_te'] = X_test[col].map(smoothed_full).fillna(global_mean)\n",
    "    \n",
    "    return X_train_te, X_test_te\n",
    "\n",
    "cat_cols_for_te = [col for col in X_full.columns if col.endswith('_cat')]\n",
    "\n",
    "if len(cat_cols_for_te) > 0:\n",
    "    print(f\"Applying conservative target encoding to {len(cat_cols_for_te)} features...\")\n",
    "    X_full, test_full = conservative_target_encode(\n",
    "        X_full, y_full, test_full,\n",
    "        cat_cols_for_te, alpha=20  # Increased from 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: TRAINING REGULARIZED MODELS\n",
      "================================================================================\n",
      "\n",
      "üìä Training CatBoost with 5-fold CV...\n",
      "  Fold 1/5...\n",
      "  Fold 2/5...\n",
      "  Fold 3/5...\n",
      "  Fold 4/5...\n",
      "  Fold 5/5...\n",
      "‚úÖ CatBoost OOF AUROC: 0.6378\n",
      "\n",
      "üìä Training LightGBM with 5-fold CV...\n",
      "  Fold 1/5...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.576338\n",
      "  Fold 2/5...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.599038\n",
      "  Fold 3/5...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.591942\n",
      "  Fold 4/5...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.598764\n",
      "  Fold 5/5...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[997]\tvalid_0's binary_logloss: 0.594691\n",
      "‚úÖ LightGBM OOF AUROC: 0.6315\n",
      "\n",
      "üìä Training XGBoost with 5-fold CV...\n",
      "  Fold 1/5...\n",
      "  Fold 2/5...\n",
      "  Fold 3/5...\n",
      "  Fold 4/5...\n",
      "  Fold 5/5...\n",
      "‚úÖ XGBoost OOF AUROC: 0.6234\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 5: REGULARIZED MODELS WITH EARLY STOPPING\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: TRAINING REGULARIZED MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use proper CV instead of single split\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Store OOF predictions\n",
    "oof_cat = np.zeros(len(X_full))\n",
    "oof_lgb = np.zeros(len(X_full))\n",
    "oof_xgb = np.zeros(len(X_full))\n",
    "\n",
    "# Store test predictions\n",
    "test_cat = np.zeros(len(test_full))\n",
    "test_lgb = np.zeros(len(test_full))\n",
    "test_xgb = np.zeros(len(test_full))\n",
    "\n",
    "# CatBoost with strong regularization\n",
    "print(\"\\nüìä Training CatBoost with 5-fold CV...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_full, y_full)):\n",
    "    print(f\"  Fold {fold+1}/5...\")\n",
    "    \n",
    "    X_tr, X_vl = X_full.iloc[train_idx], X_full.iloc[val_idx]\n",
    "    y_tr, y_vl = y_full.iloc[train_idx], y_full.iloc[val_idx]\n",
    "    \n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        depth=4,  # Reduced from 6-8 to prevent overfitting\n",
    "        learning_rate=0.02,\n",
    "        l2_leaf_reg=10,  # Strong regularization\n",
    "        bagging_temperature=1.0,\n",
    "        random_strength=2,\n",
    "        auto_class_weights='Balanced',\n",
    "        random_state=RANDOM_SEED + fold,\n",
    "        verbose=0,\n",
    "        early_stopping_rounds=50,\n",
    "        eval_metric='AUC'\n",
    "    )\n",
    "    \n",
    "    cat_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=(X_vl, y_vl),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    oof_cat[val_idx] = cat_model.predict_proba(X_vl)[:, 1]\n",
    "    test_cat += cat_model.predict_proba(test_full)[:, 1] / 5\n",
    "\n",
    "cat_cv_score = roc_auc_score(y_full, oof_cat)\n",
    "print(f\"‚úÖ CatBoost OOF AUROC: {cat_cv_score:.4f}\")\n",
    "\n",
    "# LightGBM with strong regularization\n",
    "print(\"\\nüìä Training LightGBM with 5-fold CV...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_full, y_full)):\n",
    "    print(f\"  Fold {fold+1}/5...\")\n",
    "    \n",
    "    X_tr, X_vl = X_full.iloc[train_idx], X_full.iloc[val_idx]\n",
    "    y_tr, y_vl = y_full.iloc[train_idx], y_full.iloc[val_idx]\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        num_leaves=31,  # Conservative\n",
    "        learning_rate=0.02,\n",
    "        min_child_samples=50,  # Prevent overfitting\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=1.0,  # L1 regularization\n",
    "        reg_lambda=1.0,  # L2 regularization\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED + fold,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_vl, y_vl)],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    oof_lgb[val_idx] = lgb_model.predict_proba(X_vl)[:, 1]\n",
    "    test_lgb += lgb_model.predict_proba(test_full)[:, 1] / 5\n",
    "\n",
    "lgb_cv_score = roc_auc_score(y_full, oof_lgb)\n",
    "print(f\"‚úÖ LightGBM OOF AUROC: {lgb_cv_score:.4f}\")\n",
    "\n",
    "# XGBoost with strong regularization\n",
    "print(\"\\nüìä Training XGBoost with 5-fold CV...\")\n",
    "scale_pos_weight = (y_full == 0).sum() / (y_full == 1).sum()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_full, y_full)):\n",
    "    print(f\"  Fold {fold+1}/5...\")\n",
    "    \n",
    "    X_tr, X_vl = X_full.iloc[train_idx], X_full.iloc[val_idx]\n",
    "    y_tr, y_vl = y_full.iloc[train_idx], y_full.iloc[val_idx]\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=4,  # Shallow trees\n",
    "        learning_rate=0.02,\n",
    "        min_child_weight=5,  # Prevent overfitting\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=1.0,  # Regularization\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=RANDOM_SEED + fold,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_vl, y_vl)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    oof_xgb[val_idx] = xgb_model.predict_proba(X_vl)[:, 1]\n",
    "    test_xgb += xgb_model.predict_proba(test_full)[:, 1] / 5\n",
    "\n",
    "xgb_cv_score = roc_auc_score(y_full, oof_xgb)\n",
    "print(f\"‚úÖ XGBoost OOF AUROC: {xgb_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: ENSEMBLE STRATEGIES\n",
      "================================================================================\n",
      "\n",
      "Ensemble Results (OOF scores):\n",
      "  CatBoost:          0.6378\n",
      "  LightGBM:          0.6315\n",
      "  XGBoost:           0.6234\n",
      "  Simple Average:    0.6355\n",
      "  Weighted Average:  0.6355\n",
      "  Rank Average:      0.6362\n",
      "  Linear Stacking:   0.6385\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 6: SIMPLE ENSEMBLING (AVOID OVERFITTING)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: ENSEMBLE STRATEGIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simple average\n",
    "oof_avg = (oof_cat + oof_lgb + oof_xgb) / 3\n",
    "test_avg = (test_cat + test_lgb + test_xgb) / 3\n",
    "avg_score = roc_auc_score(y_full, oof_avg)\n",
    "\n",
    "# Weighted by CV score\n",
    "total_score = cat_cv_score + lgb_cv_score + xgb_cv_score\n",
    "w_cat = cat_cv_score / total_score\n",
    "w_lgb = lgb_cv_score / total_score\n",
    "w_xgb = xgb_cv_score / total_score\n",
    "\n",
    "oof_weighted = w_cat * oof_cat + w_lgb * oof_lgb + w_xgb * oof_xgb\n",
    "test_weighted = w_cat * test_cat + w_lgb * test_lgb + w_xgb * test_xgb\n",
    "weighted_score = roc_auc_score(y_full, oof_weighted)\n",
    "\n",
    "# Rank average (most robust)\n",
    "oof_rank = (rankdata(oof_cat) + rankdata(oof_lgb) + rankdata(oof_xgb)) / 3\n",
    "test_rank = (rankdata(test_cat) + rankdata(test_lgb) + rankdata(test_xgb)) / 3\n",
    "rank_score = roc_auc_score(y_full, oof_rank)\n",
    "\n",
    "# Simple stacking with linear model (low overfitting)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "oof_stack = np.column_stack([oof_cat, oof_lgb, oof_xgb])\n",
    "test_stack = np.column_stack([test_cat, test_lgb, test_xgb])\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced', random_state=RANDOM_SEED, max_iter=1000)\n",
    "lr.fit(oof_stack, y_full)\n",
    "\n",
    "oof_lr = lr.predict_proba(oof_stack)[:, 1]\n",
    "test_lr = lr.predict_proba(test_stack)[:, 1]\n",
    "lr_score = roc_auc_score(y_full, oof_lr)\n",
    "\n",
    "print(f\"\\nEnsemble Results (OOF scores):\")\n",
    "print(f\"  CatBoost:          {cat_cv_score:.4f}\")\n",
    "print(f\"  LightGBM:          {lgb_cv_score:.4f}\")\n",
    "print(f\"  XGBoost:           {xgb_cv_score:.4f}\")\n",
    "print(f\"  Simple Average:    {avg_score:.4f}\")\n",
    "print(f\"  Weighted Average:  {weighted_score:.4f}\")\n",
    "print(f\"  Rank Average:      {rank_score:.4f}\")\n",
    "print(f\"  Linear Stacking:   {lr_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: FINAL SELECTION & SUBMISSION\n",
      "================================================================================\n",
      "\n",
      "üèÜ Best Method: Linear Stacking\n",
      "üìä OOF AUROC: 0.6385\n",
      "\n",
      "‚ö†Ô∏è  Note: OOF score is more reliable than single validation split\n",
      "   Expected Kaggle score: 0.6385 ¬± 0.003\n",
      "‚úÖ Created: submission_robust_catboost_0.6378.csv\n",
      "‚úÖ Created: submission_robust_lightgbm_0.6315.csv\n",
      "‚úÖ Created: submission_robust_xgboost_0.6234.csv\n",
      "‚úÖ Created: submission_robust_simple_average_0.6355.csv\n",
      "‚úÖ Created: submission_robust_weighted_average_0.6355.csv\n",
      "‚úÖ Created: submission_robust_rank_average_0.6362.csv\n",
      "‚úÖ Created: submission_robust_linear_stacking_0.6385.csv\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 7: SELECT BEST METHOD & CREATE SUBMISSION\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: FINAL SELECTION & SUBMISSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {\n",
    "    'CatBoost': (cat_cv_score, test_cat),\n",
    "    'LightGBM': (lgb_cv_score, test_lgb),\n",
    "    'XGBoost': (xgb_cv_score, test_xgb),\n",
    "    'Simple Average': (avg_score, test_avg),\n",
    "    'Weighted Average': (weighted_score, test_weighted),\n",
    "    'Rank Average': (rank_score, test_rank),\n",
    "    'Linear Stacking': (lr_score, test_lr)\n",
    "}\n",
    "\n",
    "# Choose method with best OOF score\n",
    "best_method = max(results, key=lambda x: results[x][0])\n",
    "best_score, best_preds = results[best_method]\n",
    "\n",
    "print(f\"\\nüèÜ Best Method: {best_method}\")\n",
    "print(f\"üìä OOF AUROC: {best_score:.4f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: OOF score is more reliable than single validation split\")\n",
    "print(f\"   Expected Kaggle score: {best_score:.4f} ¬± 0.003\")\n",
    "\n",
    "# Create submissions for all methods\n",
    "test_ids = test[ID_COL] if ID_COL in test.columns else range(len(test))\n",
    "\n",
    "for method_name, (score, predictions) in results.items():\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'target': predictions\n",
    "    })\n",
    "    \n",
    "    filename = f'submission_robust_{method_name.lower().replace(\" \", \"_\")}_{score:.4f}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Created: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: OVERFITTING DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "üìà Score Stability Analysis:\n",
      "   Best OOF Score: 0.6385\n",
      "   Previous Val Score: 0.651\n",
      "   Previous Kaggle Score: 0.642\n",
      "   Gap (Val - Kaggle): 0.009\n",
      "\n",
      "   Expected improvement: OOF scores are more reliable\n",
      "   New expected Kaggle: 0.6385 ¬± 0.003\n",
      "\n",
      "‚úÖ GOOD: Lower OOF score suggests less overfitting\n",
      "   The gap between val and kaggle should be smaller now\n",
      "\n",
      "üí° Submission Strategy:\n",
      "   1. Submit: submission_robust_linear_stacking_0.6385.csv\n",
      "   2. If score is still lower:\n",
      "      - Try 'rank_average' (most robust to distribution shift)\n",
      "      - Try 'simple_average' (less overfitting than weighted)\n",
      "   3. Monitor: OOF vs Kaggle gap should be < 0.005 now\n",
      "\n",
      "================================================================================\n",
      "‚ú® ANTI-OVERFITTING PIPELINE COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 8: VALIDATION DIAGNOSTICS\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: OVERFITTING DIAGNOSTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìà Score Stability Analysis:\")\n",
    "print(f\"   Best OOF Score: {best_score:.4f}\")\n",
    "print(f\"   Previous Val Score: 0.651\")\n",
    "print(f\"   Previous Kaggle Score: 0.642\")\n",
    "print(f\"   Gap (Val - Kaggle): 0.009\")\n",
    "print(f\"\\n   Expected improvement: OOF scores are more reliable\")\n",
    "print(f\"   New expected Kaggle: {best_score:.4f} ¬± 0.003\")\n",
    "\n",
    "if best_score < 0.651:\n",
    "    print(f\"\\n‚úÖ GOOD: Lower OOF score suggests less overfitting\")\n",
    "    print(f\"   The gap between val and kaggle should be smaller now\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Score still high - may indicate remaining overfitting\")\n",
    "\n",
    "print(f\"\\nüí° Submission Strategy:\")\n",
    "print(f\"   1. Submit: submission_robust_{best_method.lower().replace(' ', '_')}_{best_score:.4f}.csv\")\n",
    "print(f\"   2. If score is still lower:\")\n",
    "print(f\"      - Try 'rank_average' (most robust to distribution shift)\")\n",
    "print(f\"      - Try 'simple_average' (less overfitting than weighted)\")\n",
    "print(f\"   3. Monitor: OOF vs Kaggle gap should be < 0.005 now\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ú® ANTI-OVERFITTING PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
